{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "> # **Spark Core**\n",
        "\n",
        "\n",
        "***Big data analysis course***\n",
        "\n",
        "Presenters :\n",
        "*   Marziyeh Amiri\n",
        "*   Zahra Ebadi\n",
        "*   Elnaz Nasiri\n",
        "\n",
        "Guide Master : Dr Farsad Zamani\n",
        "\n",
        "Islamic Azad University،Science And Research Branch\n",
        "\n",
        "Fall 1402\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "BWkbBWofFZLG"
      },
      "id": "BWkbBWofFZLG"
    },
    {
      "cell_type": "markdown",
      "id": "bc29a0d2",
      "metadata": {
        "id": "bc29a0d2"
      },
      "source": [
        "# Actions on RDD of key-value Pairs :\n",
        "RDDs of key-value pairs support a few additional actions, which are briefly described next."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "59d1cae6",
      "metadata": {
        "id": "59d1cae6"
      },
      "source": [
        "## 1. countByKey :\n",
        "The countByKey method counts the occurrences of each unique key in the source RDD.\n",
        "It returns a Map of key-count pairs.\n",
        "\n",
        "**Scala :**\n",
        "\n",
        "\n",
        "\n",
        "```\n",
        "val pairRdd = sc.parallelize(List((\"a\", 1), (\"b\", 2), (\"c\", 3), (\"a\", 11), (\"b\", 22), (\"a\", 1)))\n",
        "val countOfEachKey = pairRdd.countByKey\n",
        "```\n",
        "\n",
        "**Python :**"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pyspark"
      ],
      "metadata": {
        "id": "EZNeytAbFTFg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "02138dcd-985b-47fa-f649-62f8b81f046f"
      },
      "id": "EZNeytAbFTFg",
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pyspark\n",
            "  Downloading pyspark-3.5.0.tar.gz (316.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m316.9/316.9 MB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.10/dist-packages (from pyspark) (0.10.9.7)\n",
            "Building wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.5.0-py2.py3-none-any.whl size=317425344 sha256=51bb0dfb9bdf67c42d5654a891441139dff385251ab9a303daa694ae028b9070\n",
            "  Stored in directory: /root/.cache/pip/wheels/41/4e/10/c2cf2467f71c678cfc8a6b9ac9241e5e44a01940da8fbb17fc\n",
            "Successfully built pyspark\n",
            "Installing collected packages: pyspark\n",
            "Successfully installed pyspark-3.5.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "4d2c0baa",
      "metadata": {
        "id": "4d2c0baa"
      },
      "outputs": [],
      "source": [
        "from pyspark import SparkContext\n",
        "\n",
        "# Create a SparkContext\n",
        "sc = SparkContext(\"local\", \"example\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "c8d55de4",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c8d55de4",
        "outputId": "b28e85dc-92f0-4ecd-f236-fee94ce9f5bd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m [('a', 3), ('b', 2), ('c', 1)]\n"
          ]
        }
      ],
      "source": [
        "# Create an RDD\n",
        "rdd = sc.parallelize([(\"a\", 1), (\"b\", 2), (\"c\", 3), (\"a\", 11), (\"b\", 22), (\"a\", 1)])\n",
        "\n",
        "# Count occurrences of each key\n",
        "result = sorted(rdd.countByKey().items())\n",
        "\n",
        "# Print the result\n",
        "print(f\"\\033[1m\",result)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8b0dd63d",
      "metadata": {
        "id": "8b0dd63d"
      },
      "source": [
        "---\n",
        "## 2. lookup :\n",
        "The lookup method takes a key as input and returns a sequence of all the values mapped to that key in the\n",
        "source RDD.\n",
        "\n",
        "**Scala :**\n",
        "\n",
        "```\n",
        "val pairRdd = sc.parallelize(List((\"a\", 1), (\"b\", 2), (\"c\", 3), (\"a\", 11), (\"b\", 22), (\"a\", 1)))\n",
        "val values = pairRdd.lookup(\"a\")\n",
        "```\n",
        "**Python :**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "eed92f2d",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eed92f2d",
        "outputId": "08524e96-cbbd-45a4-87ed-64dc43483d22"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m [1, 11, 1]\n"
          ]
        }
      ],
      "source": [
        "# Create a pair RDD\n",
        "pair_rdd = sc.parallelize([(\"a\", 1), (\"b\", 2), (\"c\", 3), (\"a\", 11), (\"b\", 22), (\"a\", 1)])\n",
        "\n",
        "# Use filter to get values for key \"a\"\n",
        "values = pair_rdd.filter(lambda x: x[0] == \"a\").values().collect()\n",
        "\n",
        "# Print the result\n",
        "print(f\"\\033[1m\",values)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0f56fe1c",
      "metadata": {
        "id": "0f56fe1c"
      },
      "source": [
        "---\n",
        "# Actions on RDD of Numeric Types :\n",
        "RDDs containing data elements of type Integer, Long, Float, or Double support a few additional actions that\n",
        "are useful for statistical analysis. The commonly used actions from this group are briefly described next.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8fb1f39c",
      "metadata": {
        "id": "8fb1f39c"
      },
      "source": [
        "## 1. mean :\n",
        "The mean method returns the average of the elements in the source RDD.\n",
        "\n",
        "**Scala :**\n",
        "\n",
        "```\n",
        "val numbersRdd = sc.parallelize(List(2, 5, 3, 1))\n",
        "val mean = numbersRdd.mean\n",
        "```\n",
        "**Python :**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "ff455609",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ff455609",
        "outputId": "b8071284-9a48-467d-a71f-1d6fb3a8ca46"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m 2.75\n"
          ]
        }
      ],
      "source": [
        "# Create an RDD of numbers\n",
        "numbers_rdd = sc.parallelize([2, 5, 3, 1])\n",
        "\n",
        "# Calculate the mean\n",
        "mean = numbers_rdd.mean()\n",
        "\n",
        "# Print the result\n",
        "print(f\"\\033[1m\",mean)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e8df8454",
      "metadata": {
        "id": "e8df8454"
      },
      "source": [
        "---\n",
        "## 2. stdev :\n",
        "The stdev method returns the standard deviation of the elements in the source RDD.\n",
        "\n",
        "**Scala :**\n",
        "\n",
        "```\n",
        "val numbersRdd = sc.parallelize(List(2, 5, 3, 1))\n",
        "val stdev = numbersRdd.stdev\n",
        "```\n",
        "\n",
        "**Python :**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "a34c9845",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a34c9845",
        "outputId": "24eb0b55-7a06-4973-bb7c-81be445d6273"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m 1.479019945774904\n"
          ]
        }
      ],
      "source": [
        "# Create an RDD of numbers\n",
        "numbers_rdd = sc.parallelize([2, 5, 3, 1])\n",
        "\n",
        "# Calculate the standard deviation\n",
        "stdev = numbers_rdd.stdev()\n",
        "\n",
        "# Print the result\n",
        "print(f\"\\033[1m\",stdev)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b0540c12",
      "metadata": {
        "id": "b0540c12"
      },
      "source": [
        "---\n",
        "## 3. sum :\n",
        "The sum method returns the sum of the elements in the source RDD.\n",
        "\n",
        "**Scala :**\n",
        "\n",
        "```\n",
        "val numbersRdd = sc.parallelize(List(2, 5, 3, 1))\n",
        "val sum = numbersRdd.sum\n",
        "```\n",
        "**Python :**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "529725e0",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "529725e0",
        "outputId": "be62fd56-808c-4d6d-d66c-bf3721c8f67d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m 11\n"
          ]
        }
      ],
      "source": [
        "# Create an RDD of numbers\n",
        "numbers_rdd = sc.parallelize([2, 5, 3, 1])\n",
        "\n",
        "# Calculate the sum\n",
        "total_sum = numbers_rdd.sum()\n",
        "\n",
        "# Print the result\n",
        "print(f\"\\033[1m\",total_sum)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "02f121ed",
      "metadata": {
        "id": "02f121ed"
      },
      "source": [
        "---\n",
        "## 4. variance :\n",
        "The variance method returns the variance of the elements in the source RDD.\n",
        "\n",
        "**Scala :**\n",
        "```\n",
        "val numbersRdd = sc.parallelize(List(2, 5, 3, 1))\n",
        "val variance = numbersRdd.variance\n",
        "```\n",
        "**Python :**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "e53229c5",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e53229c5",
        "outputId": "f384a850-a60f-43ef-e189-170978dc2efb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m 2.1875\n"
          ]
        }
      ],
      "source": [
        "# Create an RDD of numbers\n",
        "numbers_rdd = sc.parallelize([2, 5, 3, 1])\n",
        "\n",
        "# Calculate the variance\n",
        "variance = numbers_rdd.variance()\n",
        "\n",
        "# Print the result\n",
        "print(f\"\\033[1m\",variance)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "# Saving an RDD :\n",
        "Generally, after data is processed, results are saved on disk. Spark allows an application developer to save an RDD to any hadoop supported storage system.\n",
        "This section presents commonly used RDD methods to save an RDD to a file"
      ],
      "metadata": {
        "id": "GO5BMQpEB05_"
      },
      "id": "GO5BMQpEB05_"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. saveAsTextFile :\n",
        "The saveAsTextFile method saves the elements of the source RDD in the specified directory on any\n",
        "Hadoop-supported file system. Each RDD element is converted to its string representation and stored as a\n",
        "line of text.\n",
        "\n",
        "**Scala :**\n",
        "\n",
        "```\n",
        "val numbersRdd = sc.parallelize((1 to 10000).toList)\n",
        "val filteredRdd = numbersRdd filter { x => x % 1000 == 0}\n",
        "filteredRdd.saveAsTextFile(\"numbers-as-text\")\n",
        "```\n",
        "**Python :**"
      ],
      "metadata": {
        "id": "xHRDaVTkB9T1"
      },
      "id": "xHRDaVTkB9T1"
    },
    {
      "cell_type": "code",
      "source": [
        "# Create an RDD of numbers from 1 to 10000\n",
        "numbers_rdd = sc.parallelize(list(range(1, 10001)))\n",
        "\n",
        "# Filter the RDD to keep only numbers divisible by 1000\n",
        "filtered_rdd = numbers_rdd.filter(lambda x: x % 1000 == 0)\n",
        "\n",
        "# Save the filtered RDD as a text file\n",
        "filtered_rdd.saveAsTextFile(\"numbers-as-text\")"
      ],
      "metadata": {
        "id": "4ZFQtrghCB4h"
      },
      "id": "4ZFQtrghCB4h",
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "## 2. saveAsObjectFile :\n",
        "The saveAsObjectFile method saves the elements of the source RDD as serialized Java objects in the\n",
        "specified directory.\n",
        "\n",
        "**Scala :**\n",
        "\n",
        "```\n",
        "val numbersRdd = sc.parallelize((1 to 10000).toList)\n",
        "val filteredRdd = numbersRdd filter { x => x % 1000 == 0}\n",
        "filteredRdd.saveAsObjectFile(\"numbers-as-object\")\n",
        "```\n",
        "\n",
        "**Python :**"
      ],
      "metadata": {
        "id": "v4HGJzPPCnt4"
      },
      "id": "v4HGJzPPCnt4"
    },
    {
      "cell_type": "code",
      "source": [
        "# Create an RDD of numbers from 1 to 10000\n",
        "numbers_rdd = sc.parallelize(list(range(1, 10001)))\n",
        "\n",
        "# Filter the RDD to keep only numbers divisible by 1000\n",
        "filtered_rdd = numbers_rdd.filter(lambda x: x % 1000 == 0)\n",
        "\n",
        "# Save the filtered RDD as an object file\n",
        "filtered_rdd.saveAsPickleFile(\"numbers-as-object\")"
      ],
      "metadata": {
        "id": "FQDZVx2MCplB"
      },
      "id": "FQDZVx2MCplB",
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "## 3. saveAsSequenceFile :\n",
        "The saveAsSequenceFile method saves an RDD of key-value pairs in SequenceFile format. An RDD of key_value pairs can also be saved in text format using the saveAsTextFile.\n",
        "\n",
        "**Scala :**\n",
        "\n",
        "\n",
        "\n",
        "```\n",
        "val pairs = (1 to 10000).toList map {x => (x, x*2)}\n",
        "val pairsRdd = sc.parallelize(pairs)\n",
        "val filteredPairsRdd = pairsRdd filter { case (x, y) => x % 1000 ==0 }\n",
        "filteredPairsRdd.saveAsSequenceFile(\"pairs-as-sequence\")\n",
        "filteredPairsRdd.saveAsTextFile(\"pairs-as-text\")\n",
        "```\n",
        "\n",
        "\n",
        "\n",
        "**Python :**"
      ],
      "metadata": {
        "id": "BKB05lg3C144"
      },
      "id": "BKB05lg3C144"
    },
    {
      "cell_type": "code",
      "source": [
        "# Create pairs as a list\n",
        "pairs = [(x, x*2) for x in range(1, 10001)]\n",
        "\n",
        "# Create an RDD of pairs\n",
        "pairs_rdd = sc.parallelize(pairs)\n",
        "\n",
        "# Filter the RDD to keep only pairs where the first element is divisible by 1000\n",
        "filtered_pairs_rdd = pairs_rdd.filter(lambda pair: pair[0] % 1000 == 0)\n",
        "\n",
        "# Save the filtered RDD as a SequenceFile\n",
        "filtered_pairs_rdd.saveAsSequenceFile(\"pairs-as-sequence\")\n",
        "\n",
        "# Save the filtered RDD as a text file\n",
        "filtered_pairs_rdd.saveAsTextFile(\"pairs-as-text\")"
      ],
      "metadata": {
        "id": "AzaMfPn3C6ho"
      },
      "id": "AzaMfPn3C6ho",
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "# Lazy Operations :\n",
        "RDD creation and transformation methods are lazy operations. Spark does not immediately perform any\n",
        "computation when an application calls a method that return an RDD. For example, when you read a file\n",
        "from HDFS using textFile method of SparkContext, Spark does not immediately read the file from disk.\n",
        "Similarly, RDD transformations, which return a new RDD, are lazily computed. Spark just keeps track of\n",
        "transformations applied to an RDD.\n",
        "\n",
        "Let’s consider the following sample code :\n",
        "\n",
        "**Scala :**\n",
        "\n",
        "\n",
        "\n",
        "```\n",
        "val lines = sc.textFile(\"...\")\n",
        "val errorLines = lines filter { l => l.contains(\"ERROR\")}\n",
        "val warningLines = lines filter { l => l.contains(\"WARN\")}\n",
        "```\n",
        "\n",
        "\n",
        "**Python :**"
      ],
      "metadata": {
        "id": "BWkS2354c4lL"
      },
      "id": "BWkS2354c4lL"
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CEUjxY-b6Uu-",
        "outputId": "2814ed6c-5e61-44dc-b5b1-1197579c6f29"
      },
      "id": "CEUjxY-b6Uu-",
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Read the text file\n",
        "lines = sc.textFile(\"drive/MyDrive/sample_text.txt\")\n",
        "\n",
        "# Filter lines containing \"ERROR\"\n",
        "error_lines = lines.filter(lambda l: \"ERROR\" in l)\n",
        "\n",
        "# Filter lines containing \"WARN\"\n",
        "warning_lines = lines.filter(lambda l: \"WARN\" in l)\n",
        "\n",
        "# Print the results\n",
        "print(\"Lines containing 'ERROR':\")\n",
        "print(error_lines.collect())\n",
        "\n",
        "print(\"\\nLines containing 'WARN':\")\n",
        "print(warning_lines.collect())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XEkGG5oqe_v7",
        "outputId": "dbeb8d08-c562-4b29-baaa-56d738cb1634"
      },
      "id": "XEkGG5oqe_v7",
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Lines containing 'ERROR':\n",
            "['In the intricate landscape of software development, encountering an ERROR can be both perplexing and pivotal. ', 'In this symbiotic dance between detecting ERROR and heeding WARNing, software professionals forge a resilient path towards robust and error-resistant applications, ensuring a smoother journey through the intricate realm of coding challenges.']\n",
            "\n",
            "Lines containing 'WARN':\n",
            "['As developers navigate through code, the vigilant presence of WARN messages becomes crucial, serving as beacons signaling potential pitfalls. ', 'These WARN messages act as guardians, cautioning developers about impending issues and providing insights to preemptively address vulnerabilities. ', 'In this symbiotic dance between detecting ERROR and heeding WARNing, software professionals forge a resilient path towards robust and error-resistant applications, ensuring a smoother journey through the intricate realm of coding challenges.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "# Action Triggers Computation :\n",
        "Action Triggers Computation RDD transformations are computed when an application calls an action method of an RDD or saves an RDD to a storage system. Saving an RDD to a storage system is considered as an action, even though it does not return a value to the driver program.\n",
        "Spark's lazy evaluation enables efficient RDD computations. Transformations are deferred until an action is triggered, allowing Spark to optimize operations by pipelining and minimizing unnecessary data transfer. When an action is invoked, Spark recursively processes dependencies, starting from the root RDD, to generate the desired result for the driver program.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "eolnCE7dmMl9"
      },
      "id": "eolnCE7dmMl9"
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "# Caching :\n",
        "Besides storing data in memory, caching an RDD play another important role.\n",
        "by default, every time an action method is called, Spark traverses the lineage tree of an RDD and computes all the transformations to obtain the RDD whose action method was called.\n",
        "\n",
        "Consider the following example :\n",
        "\n",
        "**Scala :**\n",
        "\n",
        "```\n",
        "val logs = sc.textFile(\"path/to/log-files\")\n",
        "val errorLogs = logs filter { l => l.contains(\"ERROR\")}\n",
        "val warningLogs = logs filter { l => l.contains(\"WARN\")}\n",
        "val errorCount = errorLogs.count\n",
        "val warningCount = warningLogs.count\n",
        "```\n",
        "\n",
        "**Python :**"
      ],
      "metadata": {
        "id": "PokALD8NnwuN"
      },
      "id": "PokALD8NnwuN"
    },
    {
      "cell_type": "code",
      "source": [
        "# Read the text file\n",
        "logs = sc.textFile(\"drive/MyDrive/sample_text.txt\")\n",
        "\n",
        "# Filter logs containing \"ERROR\"\n",
        "error_logs = logs.filter(lambda l: \"ERROR\" in l)\n",
        "\n",
        "# Filter logs containing \"WARN\"\n",
        "warning_logs = logs.filter(lambda l: \"WARN\" in l)\n",
        "\n",
        "# Count the number of \"ERROR\" logs\n",
        "error_count = error_logs.count()\n",
        "\n",
        "# Count the number of \"WARN\" logs\n",
        "warning_count = warning_logs.count()\n",
        "\n",
        "# Print the results\n",
        "print(\"Number of ERROR logs:\", error_count)\n",
        "print(\"Number of WARN logs:\", warning_count)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P0W-2vs0oRG1",
        "outputId": "11c346c2-e89a-409f-f70f-aa2a8900cfa4"
      },
      "id": "P0W-2vs0oRG1",
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of ERROR logs: 2\n",
            "Number of WARN logs: 3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "The code reads log files twice due to two calls to the count action method after a single textFile method call. This example illustrates potential inefficiencies, especially in real-world applications with numerous transformations. RDD caching in Spark computes transformations up to a point and creates a checkpoint, but the benefit starts with the second action due to lazy caching. Cached RDDs enhance subsequent actions, offering faster execution, particularly beneficial for iterating over data multiple times. Spark stores RDDs in executor memory on worker nodes when cached in-memory, optimizing performance."
      ],
      "metadata": {
        "id": "5L3XFe7yrjUn"
      },
      "id": "5L3XFe7yrjUn"
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "# RDD Caching Methods :\n",
        "The RDD class provides two methods to cache an RDD: **cache** and **persist.**"
      ],
      "metadata": {
        "id": "OrN0H9_VJw8H"
      },
      "id": "OrN0H9_VJw8H"
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## 1. cache :\n",
        "The cache method stores an RDD in the memory of the executors across a cluster. It essentially materializes\n",
        "an RDD in memory.\n",
        "The example shown earlier can be optimized using the cache method as shown next :\n",
        "\n",
        "**Scala :**\n",
        "\n",
        "\n",
        "```\n",
        "val logs = sc.textFile(\"path/to/log-files\")\n",
        "val errorsAndWarnings = logs filter { l => l.contains(\"ERROR\") || l.contains(\"WARN\")}\n",
        "errorsAndWarnings.cache()\n",
        "val errorLogs = errorsAndWarnings filter { l => l.contains(\"ERROR\")}\n",
        "val warningLogs = errorsAndWarnings filter { l => l.contains(\"WARN\")}\n",
        "val errorCount = errorLogs.count\n",
        "val warningCount = warningLogs.count\n",
        "```\n",
        "\n",
        "**Python :**"
      ],
      "metadata": {
        "id": "l-U2E50MrlKx"
      },
      "id": "l-U2E50MrlKx"
    },
    {
      "cell_type": "code",
      "source": [
        "# Read the text file\n",
        "logs = sc.textFile(\"drive/MyDrive/sample_text.txt\")\n",
        "\n",
        "# Filter logs containing \"ERROR\" or \"WARN\"\n",
        "errors_and_warnings = logs.filter(lambda l: \"ERROR\" in l or \"WARN\" in l)\n",
        "\n",
        "# Cache the RDD for better performance if reused\n",
        "errors_and_warnings.cache()\n",
        "\n",
        "# Filter logs containing only \"ERROR\"\n",
        "error_logs = errors_and_warnings.filter(lambda l: \"ERROR\" in l)\n",
        "\n",
        "# Filter logs containing only \"WARN\"\n",
        "warning_logs = errors_and_warnings.filter(lambda l: \"WARN\" in l)\n",
        "\n",
        "# Count the number of \"ERROR\" logs\n",
        "error_count = error_logs.count()\n",
        "\n",
        "# Count the number of \"WARN\" logs\n",
        "warning_count = warning_logs.count()\n",
        "\n",
        "# Print the results\n",
        "print(\"Number of ERROR logs:\", error_count)\n",
        "print(\"Number of WARN logs:\", warning_count)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J8JHEXx8tJWY",
        "outputId": "ae51f177-0bf9-4167-e3b3-0aff21805638"
      },
      "id": "J8JHEXx8tJWY",
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of ERROR logs: 2\n",
            "Number of WARN logs: 3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "## 2. persist :\n",
        "The persist method is a generic version of the cache method. It allows an RDD to be stored in memory,\n",
        "disk, or both. It optionally takes a storage level as an input parameter. If persist is called without any\n",
        "parameter, its behavior is identical to that of the cache method.\n",
        "\n",
        "**Scala :**\n",
        "\n",
        "```\n",
        "val lines = sc.textFile(\"...\")\n",
        "lines.persist()\n",
        "```\n",
        "\n",
        "The persist method supports the following common storage options:\n",
        "*\t *MEMORY_ONLY* : When an application calls the persist method with the\n",
        "MEMORY_ONLY\n",
        "flag, Spark stores RDD partitions in memory on the worker nodes using deserialized Java objects. If an RDD partition does not fit in memory on a worker node, it is computed on the fly when needed.\n",
        "\n",
        "*\t *DISK_ONLY* : If persist is called with the DISK_ONLY flag, Spark materializes RDD\n",
        "partitions and stores them in a local file system on each worker node. This option\n",
        "can be used to persist intermediate RDDs so that subsequent actions do not have to\n",
        "start computation from the root RDD.\n",
        "*\t *MEMORY_AND_DISK* : In this case, Spark stores as many RDD partitions in memory as\n",
        "possible and stores the remaining partitions on disk.\n",
        "*\t *MEMORY_ONLY_SER* : In this case, Spark stores RDD partitions in memory as serialized\n",
        "Java objects. A serialized Java object consumes less memory, but is more CPU intensive to read. This option allows a trade-off between memory consumption and\n",
        "CPU utilization.\n",
        "*\t *MEMORY_AND_DISK_SER* : Spark stores in memory as serialized Java objects as many\n",
        "RDD partitions as possible. The remaining partitions are saved on disk.\n",
        "\n",
        "**Python :**"
      ],
      "metadata": {
        "id": "lURutMUGugu3"
      },
      "id": "lURutMUGugu3"
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.storagelevel import StorageLevel\n",
        "\n",
        "# Read the text file\n",
        "lines = sc.textFile(\"drive/MyDrive/sample_text.txt\")\n",
        "\n",
        "# Persist the RDD with MEMORY ONLY storage level\n",
        "lines.persist(StorageLevel.MEMORY_ONLY)\n",
        "\n",
        "# Persist the RDD with DISK ONLY storage level\n",
        "#lines.persist(StorageLevel.DISK_ONLY)\n",
        "\n",
        "# Persist the RDD with MEMORY AND DISK SER storage level\n",
        "#lines.persist(StorageLevel.MEMORY_AND_DISK)\n",
        "\n",
        "# Persist the RDD with MEMORY ONLY SER storage level\n",
        "#lines.persist(StorageLevel.MEMORY_ONLY_2)\n",
        "\n",
        "# Persist the RDD with MEMORY AND DISK SER storage level\n",
        "#lines.persist(StorageLevel.MEMORY_AND_DISK_2)\n",
        "\n",
        "# Action 1: Count the number of lines\n",
        "line_count = lines.count()\n",
        "print(f\"Number of lines: {line_count}\")\n",
        "\n",
        "# Action 2: Display the first 10 lines\n",
        "first_10_lines = lines.take(10)\n",
        "print(\"First 10 lines:\")\n",
        "for line in first_10_lines:\n",
        "    print(line)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yL-taQ0Ouy46",
        "outputId": "92786133-b055-41d2-e8f6-b53a123d0d77"
      },
      "id": "yL-taQ0Ouy46",
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of lines: 4\n",
            "First 10 lines:\n",
            "In the intricate landscape of software development, encountering an ERROR can be both perplexing and pivotal. \n",
            "As developers navigate through code, the vigilant presence of WARN messages becomes crucial, serving as beacons signaling potential pitfalls. \n",
            "These WARN messages act as guardians, cautioning developers about impending issues and providing insights to preemptively address vulnerabilities. \n",
            "In this symbiotic dance between detecting ERROR and heeding WARNing, software professionals forge a resilient path towards robust and error-resistant applications, ensuring a smoother journey through the intricate realm of coding challenges.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "### RDD Caching Is Fault Tolerant :\n",
        "A Spark application will not crash if a node with cached RDD partitions fails. Spark automatically recreates and caches the partitions stored on the failed node on another node. Spark uses RDD lineag information to recompute lost cached partitions.\n",
        "\n",
        "### Cache Memory Management :\n",
        "Spark automatically manages cache memory using LRU (least recently used) algorithm. It removes old\n",
        "RDD partitions from cache memory when needed. In addition, the RDD API includes a method called\n",
        "unpersist(). An application can call this method to manually remove RDD partitions from memory."
      ],
      "metadata": {
        "id": "pjx-jiH-6aTL"
      },
      "id": "pjx-jiH-6aTL"
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "# Spark Jobs :\n",
        "Spark applications rely on RDD operations, encompassing transformation, action, and caching methods. Jobs in Spark represent sets of computations initiated by calling action methods on RDDs, triggering computations from data reading to result retrieval. RDD caching optimizes job performance, allowing for the continuation of computations from cached points. Task stages in Spark are organized into a Directed Acyclic Graph (DAG), with shuffle boundaries determining stage grouping. Executed tasks run in parallel on nodes, prioritizing data locality for scheduling. In case of node failure, tasks are resiliently resubmitted to alternate nodes, ensuring robust task execution."
      ],
      "metadata": {
        "id": "Tm0ejtVkAxQ7"
      },
      "id": "Tm0ejtVkAxQ7"
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "# Shared Variables :\n",
        "Spark's shared-nothing architecture partitions data across nodes, with no global memory for tasks. Communication between the driver program and tasks occurs through message sharing. To optimize for efficiency, Spark sends variables to executors, but default behavior may lead to redundancy, prompting the introduction of shared variables for scenarios like updating global variables across tasks on different nodes.\n"
      ],
      "metadata": {
        "id": "unwlIrFNCNnR"
      },
      "id": "unwlIrFNCNnR"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Broadcast Variables :\n",
        "Broadcast variables in Spark optimize data sharing by sending them to worker nodes only once, cached in deserialized form in executor memory as read-only variables. This approach is particularly beneficial for multi-stage jobs and tasks referencing the same driver variable, avoiding the performance impact of deserialization before each task. The SparkContext class's broadcast method creates broadcast variables, providing efficient access to shared data across tasks in a Spark application.\n",
        "\n",
        "Consider an application where we want to generate transaction details from e-commerce transactions. In\n",
        "a real-world application, there would be a master customer table, a master item table, and transactions table.\n",
        "\n",
        "**Scala :**\n",
        "\n",
        "\n",
        "```\n",
        "case class Transaction(id: Long, custId: Int, itemId: Int)\n",
        "case class TransactionDetail(id: Long, custName: String, itemName: String)\n",
        "val customerMap = Map(1 -> \"Tom\", 2 -> \"Harry\")\n",
        "val itemMap = Map(1 -> \"Razor\", 2 -> \"Blade\")\n",
        "val transactions = sc.parallelize(List(Transaction(1, 1, 1), Transaction(2, 1, 2)))\n",
        "val bcCustomerMap = sc.broadcast(customerMap)\n",
        "val bcItemMap = sc.broadcast(itemMap)\n",
        "val transactionDetails = transactions.map{t => TransactionDetail(\n",
        " t.id, bcCustomerMap.value(t.custId), bcItemMap.value(t.itemId))}\n",
        "transactionDetails.collect\n",
        "```\n",
        "\n",
        "**Python :**"
      ],
      "metadata": {
        "id": "bP5uft4fC6Yj"
      },
      "id": "bP5uft4fC6Yj"
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the case classes\n",
        "class Transaction:\n",
        "    def __init__(self, id, custId, itemId):\n",
        "        self.id = id\n",
        "        self.custId = custId\n",
        "        self.itemId = itemId\n",
        "\n",
        "class TransactionDetail:\n",
        "    def __init__(self, id, custName, itemName):\n",
        "        self.id = id\n",
        "        self.custName = custName\n",
        "        self.itemName = itemName\n",
        "\n",
        "# Define the data\n",
        "customer_map = {1: \"Tom\", 2: \"Harry\"}\n",
        "item_map = {1: \"Razor\", 2: \"Blade\"}\n",
        "transactions_data = [(1, 1, 1), (2, 1, 2)]\n",
        "\n",
        "# Create RDDs\n",
        "transactions_rdd = sc.parallelize([Transaction(*t) for t in transactions_data])\n",
        "bc_customer_map = sc.broadcast(customer_map)\n",
        "bc_item_map = sc.broadcast(item_map)\n",
        "\n",
        "# Map the transactions to TransactionDetail\n",
        "transaction_details_rdd = transactions_rdd.map(\n",
        "    lambda t: TransactionDetail(t.id, bc_customer_map.value[t.custId], bc_item_map.value[t.itemId])\n",
        ")\n",
        "\n",
        "# Collect the results\n",
        "result = transaction_details_rdd.collect()\n",
        "\n",
        "# Print the result\n",
        "for detail in result:\n",
        "    print(f\"TransactionDetail(id={detail.id}, custName={detail.custName}, itemName={detail.itemName})\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BTk9xlleEM5Y",
        "outputId": "3f3a8fbf-af89-4a55-d976-94b67b1617b1"
      },
      "id": "BTk9xlleEM5Y",
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TransactionDetail(id=1, custName=Tom, itemName=Razor)\n",
            "TransactionDetail(id=2, custName=Tom, itemName=Blade)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Broadcast variables in Spark facilitated an efficient join between customer, item, and transaction datasets by sending data to each node only once, avoiding the network shuffle associated with the join operator in the RDD API. This approach replaced an expensive join operation with a more economical map operation."
      ],
      "metadata": {
        "id": "1biifqTaErBo"
      },
      "id": "1biifqTaErBo"
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "## 2. Accumulators :\n",
        "Accumulators in Spark are add-only variables updated by tasks on different nodes and read by the driver program, suitable for implementing counters and aggregations. The SparkContext class facilitates accumulator creation with an optional name for display in the Spark UI, providing operators like add and += for task updates. The accumulator's value can only be read by the driver program using the value method.\n",
        "\n",
        "Let’s consider an application that needs to filter out and count the number of invalid customer identifiers\n",
        "in a customer table. In a real-world application, we will read the input data from disk and write the filtered\n",
        "data back to another file disk.\n",
        "\n",
        "**Scala :**\n",
        "\n",
        "\n",
        "```\n",
        "case class Customer(id: Long, name: String)\n",
        "val customers = sc.parallelize(List(\n",
        " Customer(1, \"Tom\"),\n",
        " Customer(2, \"Harry\"),\n",
        " Customer(-1, \"Paul\")))\n",
        "val badIds = sc.accumulator(0, \"Bad id accumulator\")\n",
        "val validCustomers = customers.filter(c => if (c.id < 0) {\n",
        " badIds += 1\n",
        " false\n",
        " } else true\n",
        " )\n",
        "val validCustomerIds = validCustomers.count\n",
        "val invalidCustomerIds = badIds.value\n",
        "```\n",
        "\n",
        "**Python :**"
      ],
      "metadata": {
        "id": "7sqbZDm-EsYl"
      },
      "id": "7sqbZDm-EsYl"
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the case class\n",
        "class Customer:\n",
        "    def __init__(self, id, name):\n",
        "        self.id = id\n",
        "        self.name = name\n",
        "\n",
        "# Define the data\n",
        "customer_data = [(1, \"Tom\"), (2, \"Harry\"), (-1, \"Paul\")]\n",
        "\n",
        "# Create RDD\n",
        "customers_rdd = sc.parallelize([Customer(*c) for c in customer_data])\n",
        "\n",
        "# Filter valid customers and update the accumulator for bad ids\n",
        "def filter_and_count(c):\n",
        "    if c.id < 0:\n",
        "        return False\n",
        "    return True\n",
        "\n",
        "# Count the valid and invalid customer ids\n",
        "valid_customers_rdd = customers_rdd.filter(filter_and_count)\n",
        "valid_customer_ids = valid_customers_rdd.count()\n",
        "invalid_customer_ids = customers_rdd.count() - valid_customer_ids\n",
        "\n",
        "# Print the results\n",
        "print(f\"Valid Customer IDs: {valid_customer_ids}\")\n",
        "print(f\"Invalid Customer IDs: {invalid_customer_ids}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XIqegRv8HbXk",
        "outputId": "2f867208-d104-4888-baa9-747bafa33796"
      },
      "id": "XIqegRv8HbXk",
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Valid Customer IDs: 2\n",
            "Invalid Customer IDs: 1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "id": "998853a2",
      "metadata": {
        "id": "998853a2"
      },
      "outputs": [],
      "source": [
        "sc.stop()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Caution is advised when using Accumulators in Spark, as updates within transformations may not be executed exactly once, especially if a task or stage is re-executed. Additionally, updates are deferred until an RDD action method is called, potentially leading to incorrect values if the driver program queries the accumulator before an action is invoked."
      ],
      "metadata": {
        "id": "NGWf0ZF1Ipki"
      },
      "id": "NGWf0ZF1Ipki"
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "# Summary :\n",
        "\n",
        "\n",
        "Spark is a high-performance, fault-tolerant, and user-friendly in-memory cluster computing framework, outpacing Hadoop MapReduce by up to 100 times. With an expressive API in Java, Python, Scala, and R, Spark enhances developer productivity, offering a unified platform for diverse big data applications and supporting interactive data analysis and iterative algorithms. The programming model revolves around RDDs, resembling Scala collections and enabling distributed data processing.\n",
        "\n",
        "*THE END*\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "XqjLVcCfJOUM"
      },
      "id": "XqjLVcCfJOUM"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.4"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}